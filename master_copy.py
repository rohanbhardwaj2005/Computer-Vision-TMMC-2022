# -*- coding: utf-8 -*-
"""Master_Copy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12h6aZyMAbalXOV7BMS6dTIX5E0naCNaI

# Import libraries and Model
"""

# Commented out IPython magic to ensure Python compatibility.
# import dependencies
import os
import sys

import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import cv2

import ipywidgets as widgets
from ipywidgets import interact, interact_manual

from IPython.display import display, Javascript, Image

from google.colab.output import eval_js
from base64 import b64decode, b64encode
import PIL
import io
import html
import time
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
import matplotlib.animation as animation

#Get Python and OpenCV Version

print('OpenCV-Python Lib Version:', cv2.__version__)
print('Python Version:',sys.version)

import torch
import IPython

model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

"""# Video live stream"""

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes


  # JavaScript to properly create our live video stream using our webcam as input
def video_stream():
  js = Javascript('''
    var video;
    var div = null;
    var stream;
    var captureCanvas;
    var imgElement;
    var labelElement;

    var pendingResolve = null;
    var shutdown = false;

    function removeDom() {
       stream.getVideoTracks()[0].stop();
       video.remove();
       div.remove();
       video = null;
       div = null;
       stream = null;
       imgElement = null;
       captureCanvas = null;
       labelElement = null;
    }

    function onAnimationFrame() {
      if (!shutdown) {
        window.requestAnimationFrame(onAnimationFrame);
      }
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve;
        pendingResolve = null;
        lp(result);
      }
    }

    async function createDom() {
      if (div !== null) {
        return stream;
      }

      div = document.createElement('div');
      div.style.border = '2px solid black';
      div.style.padding = '3px';
      div.style.width = '100%';
      div.style.maxWidth = '600px';
      document.body.appendChild(div);

      const modelOut = document.createElement('div');
      modelOut.innerHTML = "<span>Status:</span>";
      labelElement = document.createElement('span');
      labelElement.innerText = 'No data';
      labelElement.style.fontWeight = 'bold';
      modelOut.appendChild(labelElement);
      div.appendChild(modelOut);

      video = document.createElement('video');
      video.style.display = 'block';
      video.width = div.clientWidth - 6;
      video.setAttribute('playsinline', '');
      video.onclick = () => { shutdown = true; };
      stream = await navigator.mediaDevices.getUserMedia(
          {video: { facingMode: "environment"}});
      div.appendChild(video);

      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute';
      imgElement.style.zIndex = 1;
      imgElement.onclick = () => { shutdown = true; };
      div.appendChild(imgElement);

      const instruction = document.createElement('div');
      instruction.innerHTML =
          '<span style="color: red; font-weight: bold;">' +
          'When finished, click here or on the video to stop this demo</span>';
      div.appendChild(instruction);
      instruction.onclick = () => { shutdown = true; };

      video.srcObject = stream;
      await video.play();

      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; //video.videoWidth;
      captureCanvas.height = 480; //video.videoHeight;
      window.requestAnimationFrame(onAnimationFrame);

      return stream;
    }
    async function stream_frame(label, imgData) {
      if (shutdown) {
        removeDom();
        shutdown = false;
        return '';
      }

      var preCreate = Date.now();
      stream = await createDom();

      var preShow = Date.now();
      if (label != "") {
        labelElement.innerHTML = label;
      }

      if (imgData != "") {
        var videoRect = video.getClientRects()[0];
        imgElement.style.top = videoRect.top + "px";
        imgElement.style.left = videoRect.left + "px";
        imgElement.style.width = videoRect.width + "px";
        imgElement.style.height = videoRect.height + "px";
        imgElement.src = imgData;
      }

      var preCapture = Date.now();
      var result = await new Promise(function(resolve, reject) {
        pendingResolve = resolve;
      });
      shutdown = false;

      return {'create': preShow - preCreate,
              'show': preCapture - preShow,
              'capture': Date.now() - preCapture,
              'img': result};
    }
    ''')

  display(js)

def video_frame(label, bbox):
  data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))
  return data

# start streaming video from webcam
video_stream()
# label for video
label_html = 'Capturing...'
# initialze bounding box to empty
bbox = ''
count = 0

metal_bar_x = 290
start_conveyor = None

# initialize the Haar Cascade face detection model
face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))

while True:
    js_reply = video_frame(label_html, bbox)
    if not js_reply:
        break

    # convert JS response to OpenCV Image
    img = js_to_image(js_reply["img"])

    results = model(img)

    frame = img

    bbox_array = np.zeros([480,640,4], dtype=np.uint8)


    for i in results.xyxy[0]:
      if i[5] == 2 or i[5] == 7:
        #print("Found")
        if(start_conveyor is None):
          start_conveyor = float(i[0])
        bbox_array = cv2.rectangle(bbox_array, (int(i[0]), int(i[1])), (int(i[2]), int(i[3])), (255,0,0), 2)
        x_min = int(i[0])
        x_max = int(i[2])
        y_min = int(i[1])
        y_max = int(i[3])
        cropped = frame[int(y_min + (y_max-y_min)/2):y_max,x_min:x_max].copy()

        imghsv = cv2.cvtColor(cropped, cv2.COLOR_BGR2HSV).astype("float32")
        (h, s, v) = cv2.split(imghsv)
        v = v**3*0.0001
        v = np.clip(v,0,255)
        imghsv = cv2.merge([h,s,v])
        cropped = cv2.cvtColor(imghsv.astype("uint8"), cv2.COLOR_HSV2BGR)
        cropped = cv2.medianBlur(cropped,5)

        resize_const = 16
        width = int(cropped.shape[1] * resize_const)
        height = int(cropped.shape[0] * resize_const)

        resized_img = cv2.resize(cropped, (width, height))
        resized_img = cv2.medianBlur(resized_img,5)
        img_edit = resized_img

        #Variables
        alpha = 3
        beta = 0.3

        # Increasing Contrast and Brightness of Image
        for y in range(resized_img.shape[0]):
          for x in range(resized_img.shape[1]):
            img_edit[y,x] = np.clip(alpha*resized_img[y,x] + beta, 0, 255)

        img_edge = cv2.Canny(img_edit,75,100)

        circles = cv2.HoughCircles(img_edge,cv2.HOUGH_GRADIENT,1.1,minDist=400,param1=200,param2=70)

        if(not(circles is None)):
          circles = np.uint16(np.around(circles))
          max_val_x = 0
          best_val_x = None
          for val in circles[0,:]:
              if(max_val_x < val[0]/resize_const):
                max_val_x = val[0]/resize_const
                best_val_x = val
                cv2.rectangle(bbox_array, (int(x_min+val[0]/resize_const),int(y_min+(val[1]+height)/resize_const)), (int(x_min+val[0]/resize_const)+int(val[2]/resize_const),int(y_min+(val[1]+height)/resize_const+int(val[2]/resize_const))),(100,100,0),2)


          if(not(best_val_x is None)):
              val = best_val_x
              if(not(start_conveyor is None)):
                dis = 55*((max_val_x/resize_const) + (x_min-start_conveyor))/(metal_bar_x-start_conveyor)
                #print(dis)
              cv2.rectangle(bbox_array, (int(x_min+val[0]/resize_const),int(y_min+(val[1]+height)/resize_const)), (int(x_min+val[0]/resize_const)+int(val[2]/resize_const),int(y_min+(val[1]+height)/resize_const+int(val[2]/resize_const))),(255,255,0),2)
          if(abs(max_val_x/resize_const + x_min - metal_bar_x) < 30):
            print("Capture")
            cv2.imwrite('screenshot.jpg', frame)

    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255
    # convert overlay of bbox into bytes
    bbox_bytes = bbox_to_bytes(bbox_array)
    # update bbox so next frame gets new overlay
    bbox = bbox_bytes

"""# Make video"""

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
import torch
import IPython

from google.colab import drive
drive.mount('/content/gdrive')

model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

def re_encode_video(video_path):
    print(f"Re-encoding video, this might take some time, please be patient.")
    #added -r 30, otherwise, for some reason it records 1000FPS as metadata for the Astra
    os.system(f"ffmpeg -y -i {video_path} -vcodec libx264 -r 30 temp.mp4")
    os.system(f"rm {video_path}")
    os.system(f"mv temp.mp4 {video_path}")
    print(f"Done encoding!")

from google.colab.patches import cv2_imshow
from IPython.display import clear_output

video_path = "/content/gdrive/MyDrive/Colab_Notebooks/Training Data/Copy of blue_lexus_1.mp4"


# Initiate video capture for video file. video.mp4 should be in your working directory
cap = cv2.VideoCapture(video_path)

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

res=(int(width), int(height))

fourcc = cv2.VideoWriter_fourcc(*'MP4V') #codec: this format should play in Chrome/Colab
out_filename = "/content/test.mp4"
out = cv2.VideoWriter(out_filename, fourcc, 30.0, res)

print(f"Processing video.")

frame = None
while cap.isOpened():

    is_success, frame = cap.read()

    if not is_success:
        break

    results = model(frame)

    for i in results.xyxy[0]:
      if i[5] == 2 or i[5] == 7:
        #print("Found")
        cv2.rectangle(frame, (int(i[0]), int(i[1])), (int(i[2]), int(i[3])), (255,0,0), 2)
        x_min = int(i[0])
        x_max = int(i[2])
        y_min = int(i[1])
        y_max = int(i[3])
        cropped = frame[int(y_min + (y_max-y_min)/2):y_max,x_min:x_max].copy()
        resize_const = 8
        width = int(cropped.shape[1] * resize_const)
        height = int(cropped.shape[0] * resize_const)

        resized_img = cv2.resize(cropped, (width, height))
        resized_img = cv2.medianBlur(resized_img,5)
        img_edge = cv2.Canny(resized_img,60,60)

        circles = cv2.HoughCircles(img_edge,cv2.HOUGH_GRADIENT,1.1,minDist=400,param1=200,param2=60)

        if(not(circles is None)):
          circles = np.uint16(np.around(circles))
          max_val_x = 0
          best_val_x = None
          for val in circles[0,:]:
              if(max_val_x < val[0]/resize_const):
                max_val_x = val[0]/resize_const
                best_val_x = val
                cv2.rectangle(frame, (int(x_min+val[0]/resize_const),int(y_min+(val[1]+height)/resize_const)), (int(x_min+val[0]/resize_const)+int(val[2]/resize_const),int(y_min+(val[1]+height)/resize_const+int(val[2]/resize_const))),(100,100,0),2)


          if(not(best_val_x is None)):
              val = best_val_x
              cv2.rectangle(frame, (int(x_min+val[0]/resize_const),int(y_min+(val[1]+height)/resize_const)), (int(x_min+val[0]/resize_const)+int(val[2]/resize_const),int(y_min+(val[1]+height)/resize_const+int(val[2]/resize_const))),(255,255,0),2)


    out.write(frame)

out.release()

print(f"Done! Saved in working directory with name {out_filename}")

cap.release()

re_encode_video(out_filename)
show_video(out_filename, video_width= 640)

"""# Random old stuff"""

#Group 1
img = cv2.imread(path_to_img)
plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)),plt.title('Input',color='c')
plt.show()

import torch
import IPython

model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

results = model(img)
results.show()
position = results.xyxy[0][0]
print(position[0:2])
x_min = int(position[0])
x_max = int(position[2])
y_min = int(position[1])
y_max = int(position[3])

print(y_min + (y_max-y_min)/2)

print(x_min)

img = cv2.imread(path_to_img ,cv2.IMREAD_UNCHANGED)
cropped = img[int(y_min + (y_max-y_min)/2):y_max,x_min:x_max].copy()


cv2.imwrite('/content/runs/detect/exp/img.jpg', cropped)


cropped_img = cv2.imread('/content/runs/detect/exp/img.jpg' ,cv2.IMREAD_UNCHANGED)
width = int(cropped_img.shape[1] * 800 / 100)
height = int(cropped_img.shape[0] * 800 / 100)

resized_img = cv2.resize(cropped_img, (width, height))

cv2.imwrite('/content/runs/detect/exp/img.jpg', resized_img)

plt.subplot(122),plt.imshow(cv2.cvtColor(resized_img,cv2.COLOR_BGR2RGB)),plt.title('Cropped',color='c')

# Group 2

position = results.xyxy[0][0]
print(position[0:2])
x_min = int(position[0])
x_max = int(position[2])
y_min = int(position[1])
y_max = int(position[3])

print(y_min + (y_max-y_min)/2)

print(x_min)

img = cv2.imread(path_to_img ,cv2.IMREAD_UNCHANGED)
cropped = img[int(y_min + (y_max-y_min)/2):y_max,x_min:x_max].copy()


cv2.imwrite('/content/img.jpg', cropped)


cropped_img = cv2.imread('/content/img.jpg' ,cv2.IMREAD_UNCHANGED)
cv2.imwrite('/content/img.jpg', cropped_img)
'''
width = int(cropped_img.shape[1] * 800 / 100)
height = int(cropped_img.shape[0] * 800 / 100)

resized_img = cv2.resize(cropped_img, (width, height))

cv2.imwrite('/content/img.jpg', resized_img)
'''
plt.subplot(122),plt.imshow(cv2.cvtColor(cropped_img,cv2.COLOR_BGR2RGB)),plt.title('Cropped',color='c')





# def detectShapes(img_path):
#     img = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)
#     _,img_Otsubin = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
#     contours,_ = cv2.findContours(img_Otsubin.copy(),1,2)
#     for num,cnt in enumerate(contours):
#         x,y,w,h = cv2.boundingRect(cnt)
#         approx = cv2.approxPolyDP(cnt,0.01*cv2.arcLength(cnt,True),True)
#         # print(num, approx)
#         if len(approx) > 10:
#             cv2.putText(img,"Circle",(int(x+w/2),int(y+h/2)),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)
#             cv2.drawContours(img,[cnt],-1,(0,255,0),2)

#     plt.figure(figsize=(20,10))
#     plt.subplot(131),plt.imshow(cv2.cvtColor(img_Otsubin,cv2.COLOR_BGR2RGB)),plt.title('Input',color='c')
#     plt.subplot(132),plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)),plt.title('Result')
#     plt.subplot(133),plt.imshow(cv2.cvtColor(cv2.imread(img_path),cv2.COLOR_BGR2RGB)),plt.title('Original')
#     plt.show()
#     return

# detectShapes('/content/runs/detect/exp/img.jpg')

def houghCircleDetector(path_to_img):

    img = cv2.imread(path_to_img, 0)
    img = cv2.medianBlur(img,5)
    img_edge = cv2.Canny(img,75,100)
    plt.subplot(121),plt.imshow(cv2.cvtColor(img_edge,cv2.COLOR_BGR2RGB)),plt.title('Input',color='c')

    circles = cv2.HoughCircles(img_edge,cv2.HOUGH_GRADIENT,1.1,minDist=200,param1=200,param2=70)
    circles = np.uint16(np.around(circles))
    for val in circles[0,:]:
        cv2.circle(img,(val[0],val[1]),val[2],(255,0,0),15)

    plt.figure(figsize=(20,10))
    plt.subplot(121),plt.imshow(cv2.cvtColor(img_edge,cv2.COLOR_BGR2RGB)),plt.title('Input',color='c')
    plt.subplot(122),plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)),plt.title('Result',color='c')
    plt.show()
    return

#houghLineDetector(path_to_img)
houghCircleDetector('/content/runs/detect/exp/img.jpg')

"""Detecting Wheel in Cropped Image"""

# def houghCircleDetector(path_to_img):
#     img = cv2.imread(path_to_img)
#     #img = cv2.medianBlur(img,3)
#     img_edge = cv2.Canny(img,100,200)

#     circles = cv2.HoughCircles(img_edge,cv2.HOUGH_GRADIENT,1.1,minDist=400,param1=10000,param2=10)
#     circles = np.uint16(np.around(circles))
#     for val in circles[0,:]:
#         cv2.circle(img,(val[0],val[1]),val[2],(255,0,0),15)

#     plt.figure(figsize=(20,10))
#     plt.subplot(121),plt.imshow(cv2.cvtColor(img_edge,cv2.COLOR_BGR2RGB)),plt.title('Input',color='c')
#     plt.subplot(122),plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)),plt.title('Result',color='c')
#     plt.show()
#     return

# houghCircleDetector('/content/img.jpg')

# Group 1 video detection
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def re_encode_video(video_path):
    print(f"Re-encoding video, this might take some time, please be patient.")
    #added -r 30, otherwise, for some reason it records 1000FPS as metadata for the Astra
    os.system(f"ffmpeg -y -i {video_path} -vcodec libx264 -r 30 temp.mp4")
    os.system(f"rm {video_path}")
    os.system(f"mv temp.mp4 {video_path}")
    print(f"Done encoding!")

from google.colab.patches import cv2_imshow
from IPython.display import clear_output

# Initiate video capture for video file. video.mp4 should be in your working directory
cap = cv2.VideoCapture(r'/content/gdrive/MyDrive/Training Images/black_lexus_1.mp4')

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

res=(int(width), int(height))

fourcc = cv2.VideoWriter_fourcc(*'MP4V') #codec: this format should play in Chrome/Colab
out_filename = "/content/gdrive/MyDrive/Training Images/black_lexus_1.mp4"
out = cv2.VideoWriter(out_filename, fourcc, 30.0, res)

print(f"Processing video.")

frame = None
while cap.isOpened():

    is_success, frame = cap.read()

    if not is_success:
        break

    #Do some image processing here. As an example, do point detection

    kernel = np.float32([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])
    processed_frame = cv2.filter2D(frame,-1,kernel)

    out.write(processed_frame) #write the processed images to an mp4 file
    # last_frame = processed_frame.copy()

out.release()

print(f"Done! Saved in working directory with name {out_filename}")
# OPTIONAL: show last processed frame.
# if True:
#   print(f"Showing last frame of video.")
#   cv2_imshow(last_frame)

cap.release()

re_encode_video("/content/gdrive/MyDrive/Training Images/black_lexus_1.mp4") #for some reason the codecs from OpenCV fourcc don't play nicely with Colab, so re-encode
show_video("/content/gdrive/MyDrive/Training Images/black_lexus_1.mp4", video_width= 640)

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);
      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)

  # get photo data
  data = eval_js('takePhoto({})'.format(quality))
  # get OpenCV format image
  img = js_to_image(data)
  # grayscale img
  #gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
  #print(gray.shape)
  # get face bounding box coordinates using Haar Cascade
  #faces = face_cascade.detectMultiScale(gray)
  # draw face bounding box on image
  #for (x,y,w,h) in faces:
  #    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
  # save image
  cv2.imwrite(filename, img)

  return filename

try:
  filename = take_photo('photo.jpg')
  print('Saved to {}'.format(filename))

  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))